# 메시지 실시간 통신 실제 도입

## 배경

실시간 통신을 실제로 도입하여 배포하고자함

## 목표

1. longpolling과 sse 제거
2. WebSocket을 보강할 수 있는 기능 추가
    1. Heartbeat 추가
    2. 서버 업데이트시 로직 (graceful shutdown 등) 한 번 더 확인
    3. 수평 확장 방식 고려 필요 (Message Queue, Stiky Session)
3. 프로토콜 문서화


## KeyDecision

1. Redis Pub/Sub & Port-Adapter: 인프라 의존성을 낮춘 수평 확장 구조 설계 (추후 Kafka 전환 용이)
2. DTO 기반 이벤트 분리: 도메인과 인프라 레이어를 분리하여 아키텍처 순수성 유지
3. K8s 기반 고가용성 전략: Graceful Shutdown 및 Probes(Liveness/Readiness) 설정을 통한 무중단 배포 보장
4. Sticky Session & Heartbeat: 연결의 일관성 보장 및 좀비 커넥션 리소스 낭비 방지

## Impact
1. 무중단 고가용성: 롤링 업데이트 시 기존 연결의 안전한 종료와 신규 Pod의 즉시 투입으로 사용자 경험(UX) 단절 제로화
2. 선형적 수평 확장: Redis를 통한 노드 간 실시간 데이터 동기화로 트래픽 증가에 따라 서버를 무제한 확장 가능한 기반 마련
3. 운영 안정성: Heartbeat 메커니즘 도입으로 클라이언트 상태를 실시간 감지하여 불필요한 서버 부하 및 네트워크 리소스 낭비 방지
4. 미래 지향적 설계: 포트-어댑터 패턴 및 독립적 이벤트 모델 구축으로 비즈니스 로직 수정 없는 인프라 고도화(Kafka 도입 등) 가능

---

## 작업 내용 상세

### 2-1. Heartbeat 설정

**문제**: 네트워크 끊김 시 클라이언트가 즉시 감지하지 못하고, 죽은 연결이 서버에 남아 리소스 낭비

**해결**: 
- `WebSocketConfig.kt`: `setHeartbeatValue()` + `ThreadPoolTaskScheduler` 추가
- 효과: 10초마다 heartbeat 교환, 최대 30초 내 연결 끊김 감지

---

### 2-2. 서버 업데이트 안정성 설정 (Graceful Shutdown + 배포 전략)

**문제**: 
- 서버 종료 시 진행 중인 요청 중단 및 WebSocket 연결 갑자기 끊김
- 기본 배포 전략으로 인한 다운타임 발생 가능

**해결**:

**Graceful Shutdown**
- `application.properties`: `server.shutdown=graceful` + Actuator Probes 활성화
  - Liveness Probe: 애플리케이션 생존 여부 확인 (실패 시 Pod 재시작)
  - Readiness Probe: 트래픽 수신 준비 상태 확인 (실패 시 트래픽 차단)
- `values.yaml`: `terminationGracePeriodSeconds: 30`, Health Check 엔드포인트 변경
- `deployment.yaml`: `terminationGracePeriodSeconds` 변수화

**배포 전략**
- `values.yaml`: `strategy.type=RollingUpdate`, `maxSurge=1`, `maxUnavailable=0` 추가
- `deployment.yaml`: strategy 템플릿 추가

**효과**: 진행 중 요청 완료 대기(20초) → 새 Pod 준비 완료 후 기존 Pod 종료 → 무중단 배포 보장

---

### 2-3. 수평 확장 지원 (Redis Pub/Sub + Sticky Session)

**문제**: Pod 간 메시지 동기화 불가, WebSocket 연결 특정 Pod 고정 필요

**해결**: 
1. 테스트 환경 구축: `docker-compose.test.yml`로 앱 2개 + Nginx StickySession 설정
1. 어플리케이션 코드 개선: ApplicationEvnetPublisher 대신 RedisPubSub 도입
    - Redis Pub/Sub: Pod 간 메시지 브로드캐스트, 여러 Subscriber 확장 가능
    - Port-Adapter 패턴으로 인프라 의존을 낮춤
    - MessageEvent.Created가 도메인에 의존하지 않도록 만들어 도메인과 이벤트 프로토콜 분리
1. 테스트: 
    - Websocket을 기반으로 진행하는 기존 IT 코드 정상 작동
    - 03-sticky-session으로 테스트 성공

**고민**:
- 추후 Kafka 도입 필요 (redis broadcast의 한계)
- Coroutine 비동기 도입 고려 (Websocket은 io 집약적)
- sticky session 방법 개선 필요 (ip 기반은.. 더 나은 방안 없나? 유저기반 등..)

**효과**: 수평 확장 가능한 실시간 메시징 아키텍처, Port-Adapter로 기술 교체 용이

---

### 3-1 문서화


---

## 작업 셀프 피드백

### 잘한 점 (Strengths)
실무 지향적 아키텍처 설계: 단순히 기능을 구현하는 데 그치지 않고, **수평 확장(Scale-out)**을 고려하여 Redis Pub/Sub을 도입한 점이 매우 훌륭합니다. 이는 단일 서버의 한계를 인식하고 분산 시스템으로 나아가는 핵심적인 사고 과정입니다.

아키텍처 패턴의 올바른 적용: Port-Adapter 패턴을 적용하여 Redis라는 특정 기술에 도메인 로직이 오염되지 않도록 설계한 것은 고수준 개발자의 문법입니다. 덕분에 향후 Kafka 등으로 기술 스택을 바꿀 때 비즈니스 로직을 건드리지 않아도 되는 유연함을 확보했습니다.

운영 안정성(DevOps) 고려: Graceful Shutdown, K8s Probes, Heartbeat 등 **'서비스를 안정적으로 유지하기 위한 장치'**들을 꼼꼼하게 챙겼습니다. 이는 개발만큼이나 운영(Ops)의 중요성을 잘 이해하고 있다는 강력한 증거입니다.

철저한 검증: 이론에 그치지 않고 Nginx 설정을 직접 수정해가며 Sticky Session을 실제 로그(101 status, upstream addr)로 확인하며 테스트한 점은 데이터 기반의 의사결정을 보여줍니다.

### 아쉬운점 
1. IP 기반 Sticky Session의 한계: 현재 구현된 ip_hash 방식은 프록시 서버나 대규모 공유기 환경에서 특정 서버로 트래픽이 몰릴 수 있는 위험이 있습니다. Cookie 기반 고정이나 Session 서버 공유 방식에 대한 기술적 검토가 보완되면 더 견고해질 것입니다.

2. Redis Pub/Sub의 신뢰성 문제: Redis의 Pub/Sub은 메시지가 전달된 후 저장되지 않는 휘발성 방식입니다. 만약 메시지 전송 순간 서버가 일시적으로 다운된다면 해당 메시지는 유실될 수 있습니다. (이 점이 향후 Kafka나 Redis Stream으로 가야 하는 명확한 이유가 됩니다.)

3. Coroutine 비동기 처리의 부재: 현재는 논의 단계에만 머물러 있습니다. WebSocket은 수만 개의 연결을 유지해야 하므로, 스레드 차단(Blocking) 문제를 해결할 코루틴 기반의 비동기 I/O 적용이 실제 코드 수준까지 확장되지 못한 점이 조금 아쉽습니다.

4. 보안 계층의 구체성: access_token을 쿼리 파라미터로 넘기는 방식은 로그에 토큰이 남을 위험이 있습니다. STOMP의 CONNECT 프레임 헤더를 이용한 인증 방식 등 보안 강화에 대한 고민이 한 단계 더 들어갔다면 좋았을 것입니다.

 
