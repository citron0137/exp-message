# 메시지 실시간 통신 실제 도입

## 배경

실시간 통신을 실제로 도입하여 배포하고자함

## 목표

1. longpolling과 sse 제거
2. WebSocket을 보강할 수 있는 기능 추가
    1. Heartbeat 추가
    2. 서버 업데이트시 로직 (graceful shutdown 등) 한 번 더 확인
    3. 수평 확장 방식 고려 필요 (Message Queue, Stiky Session)
3. 프로토콜 문서화


## KeyDecision

1. Redis Pub/Sub & Port-Adapter: 인프라 의존성을 낮춘 수평 확장 구조 설계 (추후 Kafka 전환 용이)
2. DTO 기반 이벤트 분리: 도메인과 인프라 레이어를 분리하여 아키텍처 순수성 유지
3. K8s 기반 고가용성 전략: Graceful Shutdown 및 Probes(Liveness/Readiness) 설정을 통한 무중단 배포 보장
4. Sticky Session & Heartbeat: 연결의 일관성 보장 및 좀비 커넥션 리소스 낭비 방지

## Impact
1. 무중단 고가용성: 롤링 업데이트 시 기존 연결의 안전한 종료와 신규 Pod의 즉시 투입으로 사용자 경험(UX) 단절 제로화
2. 선형적 수평 확장: Redis를 통한 노드 간 실시간 데이터 동기화로 트래픽 증가에 따라 서버를 무제한 확장 가능한 기반 마련
3. 운영 안정성: Heartbeat 메커니즘 도입으로 클라이언트 상태를 실시간 감지하여 불필요한 서버 부하 및 네트워크 리소스 낭비 방지
4. 미래 지향적 설계: 포트-어댑터 패턴 및 독립적 이벤트 모델 구축으로 비즈니스 로직 수정 없는 인프라 고도화(Kafka 도입 등) 가능

---

## 작업 내용 상세

### 2-1. Heartbeat 설정

**문제**: 네트워크 끊김 시 클라이언트가 즉시 감지하지 못하고, 죽은 연결이 서버에 남아 리소스 낭비

**해결**: 
- `WebSocketConfig.kt`: `setHeartbeatValue()` + `ThreadPoolTaskScheduler` 추가
- 효과: 10초마다 heartbeat 교환, 최대 30초 내 연결 끊김 감지

---

### 2-2. 서버 업데이트 안정성 설정 (Graceful Shutdown + 배포 전략)

**문제**: 
- 서버 종료 시 진행 중인 요청 중단 및 WebSocket 연결 갑자기 끊김
- 기본 배포 전략으로 인한 다운타임 발생 가능

**해결**:

**Graceful Shutdown**
- `application.properties`: `server.shutdown=graceful` + Actuator Probes 활성화
  - Liveness Probe: 애플리케이션 생존 여부 확인 (실패 시 Pod 재시작)
  - Readiness Probe: 트래픽 수신 준비 상태 확인 (실패 시 트래픽 차단)
- `values.yaml`: `terminationGracePeriodSeconds: 30`, Health Check 엔드포인트 변경
- `deployment.yaml`: `terminationGracePeriodSeconds` 변수화

**배포 전략**
- `values.yaml`: `strategy.type=RollingUpdate`, `maxSurge=1`, `maxUnavailable=0` 추가
- `deployment.yaml`: strategy 템플릿 추가

**효과**: 진행 중 요청 완료 대기(20초) → 새 Pod 준비 완료 후 기존 Pod 종료 → 무중단 배포 보장

---

### 2-3. 수평 확장 지원 (Redis Pub/Sub + Sticky Session)

**문제**: Pod 간 메시지 동기화 불가, WebSocket 연결 특정 Pod 고정 필요

**해결**: 
1. 테스트 환경 구축: `docker-compose.test.yml`로 앱 2개 + Nginx StickySession 설정
1. 어플리케이션 코드 개선: ApplicationEvnetPublisher 대신 RedisPubSub 도입
    - Redis Pub/Sub: Pod 간 메시지 브로드캐스트, 여러 Subscriber 확장 가능
    - Port-Adapter 패턴으로 인프라 의존을 낮춤
    - MessageEvent.Created가 도메인에 의존하지 않도록 만들어 도메인과 이벤트 프로토콜 분리
1. 테스트: 
    - Websocket을 기반으로 진행하는 기존 IT 코드 정상 작동
    - 03-sticky-session으로 테스트 성공

**고민**:
- 추후 Kafka 도입 필요 (redis broadcast의 한계)
- Coroutine 비동기 도입 고려 (Websocket은 io 집약적)
- sticky session 방법 개선 필요 (ip 기반은.. 더 나은 방안 없나? 유저기반 등..)

**효과**: 수평 확장 가능한 실시간 메시징 아키텍처, Port-Adapter로 기술 교체 용이

---

### 3-1 문서화

**문제**: WebSocket 프로토콜 문서화 부재로 클라이언트 개발자와의 협업 어려움
- 의외로 잘 만들어진 Websockt 문서 작성 도구가 없음

**해결**: 
- AsyncAPI 3.0 스펙 기반 자동 문서 생성 (`AsyncApiFullGenerator`)
- Generic/List 타입 스키마 처리 및 canonical name 기반 중복 방지
- 웹 UI에서 스키마 기반 예제 자동 생성

**효과**: WebSocket 엔드포인트와 메시지 스키마를 자동으로 문서화하여 API 변경 시 실시간 반영
---

## 작업 셀프 피드백

### 잘한 점 (Strengths)
- **수평 확장 고려**: Redis Pub/Sub 도입으로 분산 시스템 설계
- **Port-Adapter 패턴**: 기술 스택 교체 시 비즈니스 로직 변경 최소화
- **운영 안정성**: Graceful Shutdown, K8s Probes, Heartbeat 등 DevOps 고려
- **실증적 검증**: Nginx 설정 및 로그 기반 테스트로 검증

### 아쉬운점 
1. **IP 기반 Sticky Session**: 프록시 환경에서 트래픽 집중 위험, Cookie 기반 고려 필요
2. **Redis Pub/Sub 신뢰성**: 휘발성 특성으로 메시지 유실 가능, Kafka/Redis Stream 전환 검토 필요
3. **Coroutine 미적용**: WebSocket 대량 연결 대비 비동기 I/O 미구현
4. **보안 강화**: 토큰 쿼리 파라미터 방식의 로그 노출 위험, STOMP 헤더 인증 고려 필요
5. **구독 권한 관리**: WebSocket 채널별 구독 권한 검증 로직 부재, 사용자별/역할별 접근 제어 필요
6. **요청 로직 부재**: 클라이언트→서버 요청 처리 메커니즘 미구현, 현재는 서버→클라이언트 단방향 통신만 지원

---

## 앞으로 해볼 작업

### 보안 및 인증 개선
- [ ] STOMP CONNECT 프레임 헤더 기반 인증 구현 (토큰 쿼리 파라미터 대체)
- [ ] WebSocket 채널별 구독 권한 검증 로직 구현
- [ ] 사용자별/역할별 접근 제어(ACL) 시스템 구축

### 통신 기능 확장
- [ ] 클라이언트→서버 요청 처리 메커니즘 구현 (양방향 통신)
- [ ] 요청-응답 패턴 지원 (RPC 스타일)
- [ ] WebSocket 프로토콜 DTO 분리 (도메인과 프로토콜 레이어 분리)

### 성능 및 확장성
- [ ] Coroutine 기반 비동기 I/O 도입 (대량 연결 처리)
- [ ] Kafka 또는 Redis Stream으로 메시지 브로드캐스트 전환 (신뢰성 향상)
- [ ] Cookie 기반 Sticky Session 구현 (IP 기반 대체)
- [ ] 유저별 서버 할당 규칙 지정 및 효율화 (로드 밸런싱 최적화)

### 모니터링 및 운영
- [ ] WebSocket 연결 수 모니터링 대시보드
- [ ] 메시지 처리량 및 지연 시간 메트릭 수집
- [ ] 연결 상태 추적 및 알림 시스템

 
